# Vision-Language Model (VLM) Bootcamp ğŸš€

This repository explores cutting-edge Vision-Language Models (VLMs) like **Qwen 2.5-VL** and **CLIP**. Through Jupyter notebooks, we perform tasks such as embedding generation, zero-shot classification, image captioning, and object detectionâ€”all without task-specific fine-tuning.

---

## ğŸ§  Key Topics Covered

- ğŸ§© Visual Embeddings using CLIP
- ğŸ¯ Zero-Shot Classification (CLIP)
- ğŸ–¼ï¸ Image Captioning using Qwen 2.5-VL
- ğŸ” Object Detection with Qwen-VL
- ğŸ§ª Prompt Engineering for VLMs

---

## ğŸ“ Notebooks

| File                                       | Description                                 |
|--------------------------------------------|---------------------------------------------|
| `m0_Setup.ipynb`                           | Environment setup and model installation    |
| `m2_Embeddings.ipynb`                      | CLIP-based image and text embedding         |
| `m3_Zero-Shot-Classification-CLIP.ipynb`   | CLIP-based zero-shot classification         |
| `m5_Image-Captioning.ipynb`                | Image captioning using Qwen 2.5-VL          |
| `m6_Object-Detection-Using-Qwen-2.5VL.ipynb`| Object detection and VQA-style interaction  |

---

## âš™ï¸ Installation

Make sure you have:
- Python 3.8+
- CUDA-enabled GPU
- PyTorch with CUDA support

### ğŸ“¦ Install dependencies:
```bash
pip install torch torchvision transformers accelerate
pip install git+https://github.com/huggingface/transformers
pip install ipywidgets matplotlib tqdm
